
## Generalization of Results {#sec-energy-intro .unnumbered}


```{r}
#| warning: false
#| eval: true
#| echo: false
#| message: false
library(here)

source(here("scripts", "helpers.R"))
source(here("scripts", "analysis_funcs.R"))
source(here("scripts", "test_funcs.R"))
# # IMPORT AND SELECT DATA
country <- "switzerland"
suffix <- "w_mb"
metric <- c("bias", "log_bias", "nrmse")
model_name <- c("mm_mix","mm_era5","mm_swiss1k_hc" ,"mm_swiss1k","mb_nmm","mb_nems","mm_iconeu","mb_iconeu")
groupers <- c("loc_id", "model_name", "set", "year")

test_res <- readRDS(here("DB", "analysis", paste0("test_metrics_","daily", "_", country, "_",suffix,".rds"))) %>%
  summarize_tests(groupers) 

hyp_test <- tidyr::expand_grid(model_name, metric) %>% mutate(purrr::pmap_dfr(., .f = hypothesis_test_generalization, data = test_res,  p_crit = 0.05))


for (model in model_name) {
  metric <- "nrmse"
  p <- test_validate_plot(test_res, model, metric)
  htmlwidgets::saveWidget(p, file = glue::glue("../www/graph/test_{metric}_{model}.html"), selfcontained = TRUE)
}
for (model in model_name) {
  metric <- "log_bias"
  p <- test_validate_plot(test_res, model, metric)
  htmlwidgets::saveWidget(p, file = glue::glue("../www/graph/test_{metric}_{model}.html"), selfcontained = TRUE)
}
for (model in model_name) {
  metric <- "bias"
  p <- test_validate_plot(test_res, model, metric)
  htmlwidgets::saveWidget(p, file = glue::glue("../www/graph/test_{metric}_{model}.html"), selfcontained = TRUE)
}

```



\
\

::: {.callout-tip title="The Interesting Effect of Time Aggregation"} 
Some of the models may have been calibrated using MeteoSchweiz stations. If this is the case, their performance on the validation set may be inflated. To test generalizability and rule out calibration artifacts, we compare the results against a test set that is assumed to contain unseen stations. 
:::

\
\
\

### Methodology

We evaluate model performance on daily aggregated data.

#### Data sets:

- We use the original MeteoSchweiz Data set as validation sets (model tuning etc)

- We use the unseen Bodenmessnetz data set as test set

#### Expected Outcome:

The result of the test is a yes/no result. It will tell us whether or not the previously discussed model performances can be generalized.

- If positive, we can assume that the found performance apply to other locations

- If negative*, we cannot assume that the found performance holds for other locations

#### Metrics used:

- Bias (signed volume error)

- Log Bias (relative error, robust to scale)

- nRMSE (normalized root mean square error)

#### Aggregation:

Unfortunately, the test data are available only as daily sums, not hourly values. Therefore, we first aggregate the validation data to daily sums. We then assume that 1 event = 1 day and perform the metics on the daily sums. 

Finally, we group the results to a location/year basis (i.e. we have 1 value for each location x year x model). This will be our basis for the hypothesis test.

#### Hypothesis testing:

- For each model, we compare the log_bias distribution in the test set vs. the validate set.

- We use a Welchâ€™s t-test (unequal variance) to test the null hypothesis that both come from the same distribution.

- A significant result (p < 0.05) suggests the model's performance does not generalize between validation and test data.


\


### Analysis nRmse

Following, we can find the **nRMSE** error distribution for each model:


#### Hypothesis Test


```{r}
#| warning: false
#| eval: true
#| echo: false
#| message: false
hyp_test %>% dplyr::filter(metric == "nrmse") %>% 
  dplyr::select(model_name, mean_val, mean_test, mean_diff, p_welch, generalize_welch, p_t, generalize_t) %>%
  arrange(desc(generalize_welch)) %>%
  mke_DT()
```

\

#### Plots


\

```{r}
#| echo: false
#| results: asis
#| warning: false
#| message: false
for (mdl in model_name){
 #print(mdl)
  metric <- "nrmse"
  cat(glue::glue('<iframe src="../www/graph/test_{metric}_{mdl}.html" width="100%" height="500px" style="border:none;"></iframe>'))
  cat("\n")
}
```


\
\

### Analysis Log Bias

Following, we can find the **log_bias** error distribution for each model:


#### Hypothesis Test


```{r}
#| warning: false
#| eval: true
#| echo: false
#| message: false
hyp_test %>% dplyr::filter(metric == "log_bias") %>% 
  dplyr::select(model_name, mean_val, mean_test, mean_diff, p_welch, generalize_welch, p_t, generalize_t) %>%
  arrange(desc(generalize_welch)) %>%
  mke_DT()
```

\

#### Plots



```{r}
#| echo: false
#| results: asis
#| warning: false
#| message: false
for (mdl in model_name){
 #print(mdl)
  metric <- "log_bias"
  cat(glue::glue('<iframe src="../www/graph/test_{metric}_{mdl}.html" width="100%" height="500px" style="border:none;"></iframe>'))
  cat("\n")
}
```


\
\

### Analysis Bias

Following, we can find the **Bias** error distribution for each model:


#### Hypothesis Test


```{r}
#| warning: false
#| eval: true
#| echo: false
#| message: false
hyp_test %>% dplyr::filter(metric == "bias") %>% 
  dplyr::select(model_name, mean_val, mean_test, mean_diff, p_welch, generalize_welch, p_t, generalize_t) %>%
  arrange(desc(generalize_welch)) %>%
  mke_DT()
```

\

#### Plots


```{r}
#| echo: false
#| results: asis
#| warning: false
#| message: false
for (mdl in model_name){
 #print(mdl)
  metric <- "bias"
  cat(glue::glue('<iframe src="../www/graph/test_{metric}_{mdl}.html" width="100%" height="500px" style="border:none;"></iframe>'))
  cat("\n")
}
```


\
\




