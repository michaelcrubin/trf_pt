## Mixed Models {#sec-energy-intro .unnumbered}



```{r}
#| warning: false
#| eval: true
#| echo: false
#| message: false

library(here)
invisible(capture.output(source(here("scripts", "global.R"))))
invisible(capture.output(source(here("scripts", "helpers.R"))))
invisible(capture.output(source(here("scripts", "gamma_funcs.R"))))



```

::: {.callout-tip title="Aggregation Effects and the CLT"}
We want to shed light on the effect of aggregation of a large number of fields. What we do here is we distribute means. 
Let's assume $X$ was uniform or beta or any wild distribution, where extreme values are perfectly possible. If we average now $k$ of these $X_i$, then by CLT we obtain a distribution following $Normal(αθ, αθ^2/k)$. This, of course, does not correctly represent the tail risks.


:::


\
\


### Summary of Modelling

- We select the gamma-family for risk modelling.
- we treat each **policy** as $X_{i}=Gamma(α,θ_i)$; the municipality loss is the yearly **average** $Y_t=\frac{1}{k}\sum X_{i,t}$.
- This keeps crop/policy heterogeneity (different $θ_i$) and lets a shared weather factor be layered on top.
- This prevents from false blowing up n and underestimating risk.
- We only observe $Y$, the RV $X$ is a unobserved quantity



### What we’ve learned

- Averaging many fields **preserves the mean** but shrinks variance only when fields are nearly independent; even mild correlation ($ρ≈0.3–0.6$) wipes out most information.
- With very few years, the sample mean “looks fine” while SD and tails are badly under-estimated.
- the averaging of a large number $k$ of policies in our municipality shrinks the sample mean by the LLN and gives us a false confidence.


### Mixture models & the Bayesian link

- The observed $Y_t$ is a **mixture** of Gamma components (different $\theta_i$) multiplied by a latent weather factor; single-family fits miss that shape.
* Mixture likelihoods are multimodal and unstable in small samples, but Bayesian priors on weights and parameters regularise the problem and capture full uncertainty.

### Why move to Bayesian

1. Encodes the variance-inflation formula $\alpha\theta^{2}[\,\rho+(1-\rho)/k\,]$ directly through hierarchical priors.
2. Borrows strength across crops, fields and years—critical when $n\le5$.
3. Gives full predictive tails instead of point estimates, aligning with capital-at-risk needs.
4. Handles the mixture structure naturally, avoiding the pitfalls of frequentist EM fits.




### Example

I have a loss ratio uniform between 0-1. We can have n years and k fields. Lets simulate some combinations. 
The bell-curve’s variance is for the sample means.
We compare the example with n=3 and k=50 and vice versa. Both have the same amount of data (150) and both share the same mean. But the tail risk and variance if n=3/k=50 is much larger. The reason is inside-year averaging kills most randomness and all convergece to the mean by CLT. Conversely, when I have large n, then i get a good representation of different and extreme years and hence capture the true randomness. 

Which is important for us? 
Clearly the wider. Because we cannot assume that we insure entire municipalities. We insure fields.

```{r, fig.height=10, fig.width=10}
#| warning: false
#| eval: true
#| echo: true
#| message: false
library(here)
invisible(capture.output(source(here("scripts", "global.R"))))

CLT_sim_plot <- function(n, k){
  set.seed(42)
  # simulate raw draws and yearly averages
  raw_draws <- runif(n * k) 
  year_id   <- rep(1:n, each = k)
  avg_year  <- tapply(raw_draws, year_id, mean)

  df_raw <- data.frame(
  value = raw_draws,
  year  = factor(year_id))

  df_avg <- data.frame(
    value = avg_year,
    what  = "Yearly average"
  )
  
  mu <- 0.5
  sigma_hat <- sqrt(1/12 / k) 
  x_grid <- seq(0, 1, length.out = 400)
  df_norm <- data.frame(
    x = x_grid,
    pdf = dnorm(x_grid, mu, sigma_hat)
  )

  p<-ggplot() +
  geom_histogram(data = df_raw, aes(value,  after_stat(density)), binwidth = 0.05, alpha = 0.4, size=0, position = "identity", show.legend = FALSE) +
  geom_density(data = df_avg, aes(value, after_stat(density)), colour = "steelblue", size   = 1.2, show.legend = FALSE) +
  geom_line(data = df_norm, aes(x, pdf), colour   = "red",linetype = "dashed", size = 1, show.legend = FALSE) +
  labs(title    = sprintf("k = %d per year, n = %d", k, n),
       x = "loss ratio",
       y = "density",
       fill   = "Year",
       colour = "Year") +
  theme_minimal() + theme(legend.position="none")
  return(p)
}

p1 <- CLT_sim_plot(n= 3, k= 3)
p2 <- CLT_sim_plot(n= 50, k= 3)
p3 <- CLT_sim_plot(n= 3, k= 50)
p4 <- CLT_sim_plot(n= 50, k= 50)

combined_plot <- (p1 + p2) / (p3 + p4) +
  plot_annotation(
    title = "Coloured bars: years | Blue: empirical density | Red dashed: CLT Normal",
    theme = theme(
      legend.position="none",
      plot.title = element_text(
        hjust = 0.5, 

      )
    )
  )
combined_plot
```


