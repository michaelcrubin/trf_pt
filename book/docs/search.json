[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Tarification Portugal",
    "section": "",
    "text": "Why this matters\nAgriculture is a vital economic sector in Portugal, yet many farms remain heavily underinsured. As climate change intensifies, extreme weather events pose ever-greater threats to crop yields and rural livelihoods. Farmers face rising uncertainty and high potential losses, making reliable protection more crucial than ever.\nDespite the growing risks, there are currently few high-quality agricultural insurance products available in the market. Schweizer Hagel Portugal aims to fill this gap by leveraging its agronomic expertise, actuarial experience, and data‐science capabilities. Our mission is to strengthen the resilience of Portugal’s food production, ensuring farmers have access to insurance that truly safeguards them against climate‐driven losses.",
    "crumbs": [
      "Why this matters"
    ]
  },
  {
    "objectID": "intro/introduction.html",
    "href": "intro/introduction.html",
    "title": "Introduction",
    "section": "",
    "text": "Project Goal\nThe objective of this project is to close the protection gap in Portugal by developing a complete, data‐driven risk tariff system for Schweizer Hagel Portugal. We must produce a separate tariff for each combination of crop and municipality. Since there are 94 crops and 274 municipalities, this amounts to 94 * 274 =25’756 distinct tariffs.\n\n\nData Basis\nOur modeling relies on three main data sources: - Public damage data: Annual losses reported for each crop–municipality pair.\n- Weather data: Historical meteorological observations relevant to agricultural risk.\n- Single‐policy data: Limited policy‐level information to help infer underlying risk at the field level.\nThese datasets are extremely scarce, as illustrated in Chapter “Data Inspection.” In particular, the public damage data are sparse, with many crop–municipality combinations having few or no reported losses over time.\n\n\nAggregated Data\nUnlike markets such as Switzerland—where policy‐level losses are available—in Portugal we only observe aggregated damage outcomes. In other words, we see the result of some unknown aggregation function applied to field‐level losses, rather than the field‐level losses themselves. This aggregation has important implications: - The observed values do not directly represent our individual risk units (i.e., single fields or farms).\n- We must model how individual risks combine to produce the aggregated damage figures.\n- Careful attention to aggregation bias is crucial to obtain accurate tariffs.\n\n\nCareful Risk Modelling\nGiven the severe data limitations, short historical record, and rapidly growing exposure for Schweizer Hagel Portugal, it is critical to understand true underlying risks. In particular: - Few data: Many crop–municipality pairs have zero or minimal loss records.\n- Short history: Only a limited number of years of observations are available.\n- Growing exposure: As Schweizer Hagel Portugal expands, exposure to risk multiplies, making small errors in estimation potentially very costly.\nTo address these challenges, we develop a robust theoretical model that serves as a foundation for tarification.\n\n\nApproach & Methodology\n\nInspect the data: Carefully examine the aggregated damage and policy data to identify patterns, gaps, and heteroskedasticity.\n\nUnderstand data generation: Determine how policy‐level losses and weather factors combine to produce the public aggregates.\n\nBuild a theoretical model: Use domain expertise in agronomy and actuarial science to specify a plausible distributional form for policy‐level risk.\n\nSimulate with Monte Carlo: Run simulations under the theoretical model to study how aggregated data emerge from individual components.\n\nEstimate dependence: Leverage weather data to quantify dependence among fields in the same municipality.\n\nIncorporate single‐policy insights: Use available policy‐level data to infer the shape of the risk distribution at the atomic (field) level, rather than relying solely on aggregates.\n\n…then using some model either bayesian or other\nBy combining these steps, we aim to produce reliable, data‐driven tariffs for all 25,756 crop–municipality combinations.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "inspection/inspect_intro.html",
    "href": "inspection/inspect_intro.html",
    "title": "Data Inspection",
    "section": "",
    "text": "Summary\n\n\n\nThis chapter inspects the data and highlights two key challenges: data scarcity and aggregation. We must recognize that most of the distribution is unseen, and that we only observe aggregated effects rather than true underlying phenomena.\n\n\n\n\n\nOur data is very scarce, which means we must be particularly careful in how we inspect it. In any statistical context, good analysis accounts not only for observed values but also for the unseen portion of the distribution. In this problem, the unseen data actually constitutes the majority of what we need to model—making thorough inspection even more critical.\nA further complication is that we only have aggregated data at hand. We do not observe the true natural phenomena directly; instead, we see effects that have been pooled across regions or sums insured. Understanding how aggregation distorts the underlying signal is therefore essential before any modeling can begin.",
    "crumbs": [
      "Data Inspection"
    ]
  },
  {
    "objectID": "inspection/data_space.html",
    "href": "inspection/data_space.html",
    "title": "Data Space",
    "section": "",
    "text": "Summary\n\n\n\nAs an initial step, we want to understand the data we have available.\nWe can conceptualize these data as a data space, where each variable is a dimension. Hence:\n\nWe have 278 municipalities\nWe have 93 crops\nWe have 10 years\nOur data set has 2127 entries\n\nWhich means that our data space is 258540. Our data set covers 0.823% of the data space.\nWith other words, we have very sparse data.\n\n\n\n\n\n\nCrop Distribution\nWe can see that, out of the 49 crops, only some 20 have more than 20 data entries.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nYear Distribution\nThe years, on the other hand, are relatively well distributed.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGeographic Distribution\n\nDistribution by Municipality\n\n\n\n\n\n\n\n\n\n\n\n\n\nDistribution by District\n\n\n\n\n\n\n\n\n\n\n\n\n\nDistribution by Agro Region\n\n\n\n\n\n\n\n\n\n\n\n\n\nDistribution on a Map\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nData Space\nThe subsequent cubes give us a visual idea about the densities of these data and how they behave on geographic aggregation.\n\n\n\nOn Municipality Level\n\n\n\n\n\n\nOn District Level\n\n\n\n\n\n\nOn Agro Region Level",
    "crumbs": [
      "Data Inspection",
      "Data Space"
    ]
  },
  {
    "objectID": "inspection/data_inspect.html",
    "href": "inspection/data_inspect.html",
    "title": "Data Inspection (Burn)",
    "section": "",
    "text": "Summary\n\n\n\nWe begin with an exploratory look at the burn cost data to understand its structure and challenges. This includes assessing the overall distribution, slicing it into interpretable segments, and analyzing patterns across crops and years.\n\n\n\n\nFirst, we take a look at the raw data - specifically, the burn cost, defined as:\n\\[\nburn\\_cost = \\frac{tot\\_claims}{sum\\_insured}\n\\]\nThis inspection is done at the municipality–crop level, exactly as the data is received. The goal is to form an intuitive understanding of the distribution of the brun cost.\n\n\nOverall Burn Cost Distribution\nWe first visualize the burn cost across the entire dataset. The distribution is heavily zero-inflated, meaning there is a large number of observations with a burn cost of exactly \\(0\\). As this makes the visualization of the none-zero entries impossible, we create 4 different histogram-plots slicing different ranges of x. It’s like zooming in.\n\n\n\n\n\n\n\n\n\n\nProbability by Slice\nThe table below summarizes how many observations fall into each of the defined burn cost segments. These “slices” are intended to reveal different regimes of behavior in the data:\n\n0_zero_inflater: Exactly zero\n1_expo_decay: Small positive values (\\(0 &lt; x &lt; 0.15\\))\n2_natural_phenomena: Mid-range values (\\(0.15 &lt; x &lt; 0.99\\))\n3_damage_ceiling: Values very close to or exactly \\(1\\)\n\n\n\nslicenpercent0_zero_inflater256,51199.1961_expo_decay7260.2812_natural_phenomena1,3070.5053_damage_ceiling460.018\n\n\n\n\n\nCrop-Specific Distributions\nWe now look at a few major crops (apple, pear, cherry, corn) individually to check whether the shape of the burn cost distribution varies across them.\n\nApple\n\n\n\n\n\n\n\n\n\n\n\nPear\n\n\n\n\n\n\n\n\n\n\n\nCorn\n\n\n\n\n\n\n\n\n\n\n\nCherry\n\n\n\n\n\n\n\n\n\n\n\n\nDo Years Make a Difference?\nTo check for temporal variation, we split the data by year and examine only values in the interval \\(0.1 &lt; x &lt; 0.99\\), where most natural claim behavior is expected to occur.\n\n\n\n\n\n\n\n\n\n\n\nKey Takeaways\nThe data shows a complex and mixed distribution, with multiple components:\n\nA strong spike at \\(x = 0\\) (no claims).\nA decay-like behavior for \\(x \\in (0, 0.15)\\) — possibly exponential.\nA second spike at \\(x = 1\\), suggesting capped or full-coverage claims.\nA smooth, probabilistic structure between \\(x \\in [0.15, 1)\\), resembling Beta, Gamma, or Lognormal shapes.\n\nNotably, this structure is consistent across different crops and years, which gives us confidence to model it with a unified theoretical approach.\nBelow is a conceptual sketch showing the combined model behavior:",
    "crumbs": [
      "Data Inspection",
      "Data Inspection (Burn)"
    ]
  },
  {
    "objectID": "inspection/damage_ceiling.html",
    "href": "inspection/damage_ceiling.html",
    "title": "Damage Ceiling",
    "section": "",
    "text": "Summary\n\n\n\nWe investigate the spike at \\(x = 1\\) in the burn cost data. This section provides a theoretical explanation, supports it with empirical evidence, and concludes that this peak represents a natural upper bound — the burn ceiling — which should be explicitly modeled.\n\n\n\n\nThis section explores the peak at \\(x = 1\\) observed in the burn cost data. While much of the distribution appears continuous or decaying, this spike suggests a structural limit in how losses occur — specifically, a maximum possible burn of 1.\n\n\nTheoretical Concept\nIn typical natural processes, losses might follow a Gamma or Exponential distribution — both of which have a tail that extends to infinity. However, for crop losses, there is a hard ceiling at burn cost = 1.\nOnce a field is fully destroyed, no additional loss can occur — even if the damaging event (e.g. hail, drought) continues. In effect, all excess probability mass that would fall above \\(x = 1\\) must be accumulated at 1.\nThis leads to the formation of a spike at \\(x = 1\\), as shown in the conceptual model below:\n\n\n\n\nDoes the Probability Mass Match?\nExcluding the zero-inflated component, we observe that approximately 10%–20% of the empirical distribution’s probability mass lies at exactly \\(x = 1\\).\nWe can compare this to a Gamma distribution. For example:\n\nLet \\(X \\sim \\text{Gamma}(\\alpha = 1, \\theta = 0.3)\\)\n\nThen \\(\\mathbb{P}(X &gt; 1) \\approx 15\\%\\)\n\nThis matches quite intuitively. A Gamma with \\(\\theta = 0.3\\) is a reasonable approximation for crop-level burn rates, and this theoretical “overflow” mass aligns with the spike we observe at 1.\n\n\n\n\nHypothesis Testing\nIf this concept was true, then crop × municipality combinations with higher mean values would, in tendency, also have higher mass peaking at \\(x = 1\\). We can test this by grouping the data into crop × municipality combinations into such with P Mass at and such without the same phenomena. Then we simply take the mean of each group and compare them.\n\n\nShow Code\ntbl &lt;- data %&gt;%\n  dplyr::filter(burn_cost &gt; 0.15)%&gt;%\n  mutate(mass_above_1 = as.integer(burn_cost &gt;= 1)) %&gt;%\n  group_by(crop_key, municipality_name) %&gt;%\n  summarise(mass_above_1 = sum(mass_above_1),\n            has_above_1 = mass_above_1 &gt; 0,\n            n=n(),\n            mean_bc = mean(burn_cost), \n            .groups = \"drop\") %&gt;% \n  group_by(has_above_1) %&gt;%\n  summarise(mean_bc = mean(mean_bc), n=n(), .groups = \"drop\")\n\ntbl %&gt;%\n  flextable() %&gt;%\n  theme_vanilla()\n\n\nhas_above_1mean_bcnFALSE0.3280145328TRUE0.591642218\n\n\n\n\nResult\n\n\n\n\n\n\nPeak at \\(x = 1\\)\n\n\n\nThe crop × municipality combinations, which do not have any P Mass at \\(x = 1\\) have a mean value of 0.328, while the cells which do have P Mass at \\(x = 1\\) show a mean value of 0.592. This is a large difference. Hence, we assume that the peak at \\(x = 1\\) is indeed caused by the ceiling of the burn distribution.",
    "crumbs": [
      "Data Inspection",
      "Damage Ceiling"
    ]
  },
  {
    "objectID": "inspection/aggregation_effect.html",
    "href": "inspection/aggregation_effect.html",
    "title": "Geographic Aggregation",
    "section": "",
    "text": "Summary\n\n\n\nWe explore how geographic aggregation — from municipality to district to country level — affects the shape of the burn cost distribution. This helps us understand the source of small non-zero values.\n\n\nTo better understand why there are so many data points close to zero but not exactly zero, we analyze how the data behaves when aggregated at different geographic levels.\n\n\nAggregated to District Level\nWe recreate the earlier range plots, but now aggregate the data to the district level. That is, we have one entry per district–year–crop combination.\n\n\n\n\n\n\n\n\n\n\n\nAggregated to Country Level\nWe further aggregate the data to the country level, producing just one entry for each year and crop.\n\n\n\n\n\n\n\n\n\n\n\nAggregation Effect\nThe following plots compare the distribution of positive burn values across three levels of geographic aggregation:\n\nMunicipality level (blue)\nDistrict level (green, for a high-damage crop)\nCountry level (red, for one specific year)\n\nThis helps us visualize what happens to the distribution — especially the small-damage region — as we aggregate more.\n\n\n\n\n\n\n\n\n\n\n\nReasoning\n\n\n\n\n\n\nWhy do we have so many small damages\n\n\n\nIn some way, it doesn’t make sense to have claims smaller than 1% — no farmer would bother claiming that. The likely explanation is aggregation dilution: when a single event affects a small part of a large area, the burn cost gets spread thinly over the full insured area. As we move from municipality to district to country, this effect should become more visible. The charts above suggest this pattern — especially for all crops combined — but it’s not entirely conclusive. We will test this hypothesis further in the next section.",
    "crumbs": [
      "Data Inspection",
      "Geographic Aggregation"
    ]
  },
  {
    "objectID": "inspection/dilution_effect.html",
    "href": "inspection/dilution_effect.html",
    "title": "Dilution Effect",
    "section": "",
    "text": "Summary\n\n\n\nWe investigate whether the large number of small—but nonzero—burn cost values can be explained by a dilution effect. In other words, small burn costs may result when actual losses are spread over very large insured amounts or large geographic areas.\n\n\nA substantial fraction of burn costs falls between 0 % and 1 %. Such tiny damages seem unlikely as individual claims—few policyholders would report losses that small in isolation. We therefore hypothesize a dilution effect, meaning that real damage exists but gets “diluted” when spread across large sums insured or extensive geographic areas.\nTo test this hypothesis, we use two proxies for aggregation scale:\n\nSum insured\nGeographic area\n\n\n\nDilution Effect on Sum Insured\nFirst, we visualize the raw relationship between burn cost and sum insured. The scatterplot shows that there is no obvious linear correlation—this is expected, since both variables exhibit strong heteroskedasticity.\n\n\n\n\n\n\n\n\n\nNext, we slice burn cost by quantiles of sum insured. In other words, we group policies into bins by sum insured and then compare the burn cost distributions within each bin. We choose five bins (quantiles at 20 %, 40 %, 60 %, 80 %, 100 %). We see that as the sum insured increases, the “center of mass” of the burn cost distribution shifts left—indicating smaller relative damages on larger policies.\n\n\n\n\n\n\n\n\n\nTo display this more succinctly, we use box plots of burn cost across deciles of sum insured. These box plots confirm the trend: burn cost tends to be lower (and more tightly clustered near zero) in the higher-insured groups.\n\n\n\n\n\n\n\n\n\n\n\n\nDilution Effect on Geographic Area\nNext, we examine whether a similar dilution effect occurs when damage is spread over larger geographic areas. We merge our burn dataset with municipality area information and then plot burn cost versus area. As before, the scatterplot alone does not reveal a clear trend, due to heteroskedasticity.\n\n\n\n\n\n\n\n\n\nWe then slice burn cost by quantiles of geographic area (again using five bins). The histograms show a shift toward lower burn costs as area increases, but this effect is much weaker than what we observed for sum insured.\n\n\n\n\n\n\n\n\n\nBox plots of burn cost by decile of area confirm the finding: there is only a slight tendency for burn cost to decrease as area grows. Overall, the dilution effect on area is negligible compared to the effect on sum insured.\n\n\n\n\n\n\n\n\n\n\n\n\nResults\n\n\n\n\n\n\nBehavior of the Dilution Effect\n\n\n\nThere is a strong dilution effect with respect to sum insured, whereas the effect of geographic area is negligible or much weaker. We conclude that the clustering of burn cost values near zero primarily arises from dilution over large policy amounts, rather than being a natural effect. Now that we have established this dilution effect, we proceed to explore its statistical implications in the next chapter.",
    "crumbs": [
      "Data Inspection",
      "Dilution Effect"
    ]
  },
  {
    "objectID": "modelling/modelling_intro.html",
    "href": "modelling/modelling_intro.html",
    "title": "Data Modelling",
    "section": "",
    "text": "Summary\n\n\n\nThis chapter develops the theoretical framework for modeling highly uncertain, tail-heavy data using aggregated observations. We focus on extreme deviations in the tails and employ Monte Carlo simulations to build a coherent aggregated model.\n\n\n\n\n\nBecause our data is very scarce and concentrated in extremes, we face a dual challenge: (1) the rare events in the tail drive the largest damages but are also the most uncertain, and (2) we must build a model using only aggregated observations. Tail probabilities become especially unreliable when data is limited, so choosing appropriate distributional forms is critical.\nTo address these challenges, we propose a theoretical foundation that accounts for extreme tail behavior under aggregation. We will construct an aggregated model—one that can reproduce observed summaries and capture the unseen tail risk. Monte Carlo simulations play a central role: by generating many synthetic realizations under candidate distributions, we gain insight into how different assumptions affect tail estimates and overall risk.",
    "crumbs": [
      "Data Modelling"
    ]
  },
  {
    "objectID": "modelling/gamma_family.html",
    "href": "modelling/gamma_family.html",
    "title": "Gamma Family",
    "section": "",
    "text": "Risk Distribution Function\nOur risk is governed by a probability density function \\(f\\) with an unknown parameter \\(θ\\). We want to choose a family of distributions that satisfies the following properties:\n\nSupports strictly positive values (i.e., \\(x &gt; 0\\))\nIs not necessarily bounded by 1 (we can truncate later, if needed)\nIs continuous\nCan be aggregated (under sum or average)\nHas a center of mass that may lie near zero or at some \\(x\\) between 0 and 1\n\nBy aggregation we mean taking the sum or the mean of multiple independent random variables of \\(f\\).\nIn light of actuarial and agronomic considerations, we select the Gamma family for \\(f\\).\n\n\n\n\nGamma Family\nThe Gamma family is convenient because it naturally arises as the aggregation of exponential components. Specifically:\n\nThe sum of \\(k\\) independent \\(Exp(λ=1/θ)\\) variables follows an \\(Erlang(k, θ)\\) distribution\nAn \\(Erlang(k, θ)\\) is equivalent to \\(Gamma(α = k, θ)\\) for k strictly integer\nAn \\(Exp(1/θ)\\) is a special case of \\(Gamma(α = 1, θ)\\)\nMore generally, if we hypothesize that the atomic risk follows \\(Gamma(α, θ)\\) with shape parameter \\(α\\) and scale \\(θ\\), then the sum of \\(k\\) independent copies of that atomic risk unit satisfies:\n\n\\[k * Gamma(α, θ)\\Longrightarrow Gamma(α*k, θ)\\]\n\n\nVisual Illustration\nBelow we provide a visual comparison of the exponential and Erlang (a special case of Gamma) distributions under the same scale parameter \\(θ = 0.3\\):\nThese plots illustrate how summing or averaging exponential/Gamma variables shifts and reshapes the density.",
    "crumbs": [
      "Data Modelling",
      "Gamma Family"
    ]
  },
  {
    "objectID": "modelling/risk_model.html",
    "href": "modelling/risk_model.html",
    "title": "Risk Model",
    "section": "",
    "text": "Atomic Risk Unit\nFirst, we define the practical unit we are modelling. We are interested in risks of crop losses from insurance policies. One unit of our risk is typically a single policy or field. Hence we define one atomic risk unit to be one insurance policy.\nNote that this is a quantity that does not appear explicitly in our data. We only see an aggregation thereof.\n\n\nTheoretical Risk Model\nWe defined that that our atomic risk unit follows a probability density function \\(f\\) with an unknown parameter \\(θ\\) and we selected the Gamma family for \\(f\\).\nWe face two options for our theoretical Risk Model:\n\nSingle-RV model – treat every policy × year outcome in a municipality as a draw from one common random variable \\(X\\) with a single scale \\(θ\\). Thus, the RV \\(X\\) would correspond to our observed data at municipality level.\nField-specific model – give each policy its own random variable \\(X_i\\); a year contributes one draw for \\(X_i\\). We then construct another RV \\(Y\\), which aggregates all \\(X_i\\) and corresponds to our observed data at municipality level.\n\nWhile Option 1 would be easier, it will mislead us for the following reasons:\n\nUsing a single \\(θ\\) across all risk units would introduce a systemic bias with high-risk fields being underpriced.\nJoining all policies and years into one RV would blow up n without giving much independent information and, thus, leads to false precision.\nDependence of policies not correctly modeled and potentially underestimates the tail-risks.\n\nConclusion: adopt option 2. Note that we are talking here about abstract policies/risk units, which we do not know from our data.\n\n\n\n\n\n\nRisk Model\n\n\n\nLet \\(X\\) be a policy-level burn rate: \\(X∼Gamma(α,θ)\\) with mean \\(E[X]\\) and variance \\(αθ^2\\). For a municipality containing \\(k\\) independent, identical policies in a given year, the average loss \\[\nY_t=\\frac{1}{k}\\sum_{i=1}^k X_{it}\n\\] is \\(Y∼Gamma(kα,θ/k)\\) with mean \\(αθ\\) and shrinking variance \\(αθ^2/k\\).\nWe observe \\(n\\) yearly values of \\(Y\\) and must estimate \\(α, θ\\).\n\n\n\n\nPractical Risk Model\nIn our real world, the policies \\(X_i\\) are not independent. In fact, they tend to be strongly dependent at least for the same perils. Intuitively, once it’s dry, most drought policies in the region are affected, but only few flooding policies.\nHence, we introduce the parameter \\(ρ\\in[0,1]\\) for dependence.\nIf \\(ρ=0\\), all policies are fully independent and we have \\(Y∼Gamma(kα,θ/k)\\) with mean \\(αθ\\) and variance \\(αθ^2/k\\).\nIf \\(ρ=1\\), all policies are fully dependent and we have \\(X_{1t}=X_{2t}=...=X_{kt}\\), hence \\(Y_{t} = X_{it}\\) with mean \\(αθ\\) and variance \\(αθ^2\\). We have only one RV.\nFor anything between \\([0,1]\\), we have Gamma mixtures.\n\n\nDependence & Information\nOne of the most important goals of this endavour is to gain information. We have small samples and potentially high tail risks. Hence, it’s important to squeeze out every bit of information.\nIf we have independent policies, the variance, which is the inverse of information, shrinks from \\(αθ^2\\) to \\(αθ^2/k\\), thus information \\(I\\) grows by the rate of \\(1\\sqrt{k}\\).\nIf we have a \\(ρ&gt;0\\) we have: \\[\nVar(Y)=αθ^2*(ρ+\\frac{1-ρ}{k})\n\\]\nThus the maximum information \\(I_{max} = 1/Var(Y)\\) erodes with dependence following \\((ρ+(1-ρ)/k)^{-1/2}\\)\nWe can simulate this for some settings:\n\n\nShow Code\nlibrary(here)\ninvisible(capture.output(source(here(\"scripts\", \"global.R\"))))\na &lt;- 1.5\nt &lt;- 0.3\nk &lt;- c(1, 2, 4, 8, 16, 32, 64, 128)\nr &lt;- seq(0,1, by=0.01)\n\ndependent_var &lt;- function(a, t, k, r){\n  a*t^2*(r+(1-r)/k)\n}\n\nX &lt;- tidyr::expand_grid(a, t, k, r) %&gt;%\n  mutate(var = purrr::pmap_dbl(., .f=dependent_var)) %&gt;%\n  mutate(I_max = 1/var) %&gt;%\n  mutate(factor = (r+(1-r)/k)^(-1/2)) %&gt;%\n  ggplot(aes(x = r,color = factor(k), group = k)) +\n  geom_line( aes(y = factor), size = 1) +\n\n  labs(\n    x = \"ρ\",\n    y = \"I erosion factor\",\n    color = \"k (n policies)\",\n    title = \"Eroding Information I by dependence ρ\"\n  ) +\n  theme_minimal(base_size = 13)\n\n\n\n\n\n\n\n\nDepencende erodes Information\n\n\n\nWe can say that as ~\\(ρ&gt;0.5\\), almost all additional information from large sample size has vanished and approaches the information of 1 sample. Mis-specifying \\(ρ=0\\) when it’s large would introduce a false precision.\nIn practical terms, this means that for a municipality with highly uniform risk exposure (e.g. mostly the same perils), we face a significant risk of underestimating the risk. This justified the case for our careful modelling choice.",
    "crumbs": [
      "Data Modelling",
      "Risk Model"
    ]
  },
  {
    "objectID": "modelling/gamma_theory.html",
    "href": "modelling/gamma_theory.html",
    "title": "Aggretation of Gammas",
    "section": "",
    "text": "Independent policies provide certainty\n\n\n\nThe Monte Carlo simulation confirms that, if we average \\(k\\) risk units of \\(X∼Gamma(α,θ)\\), our resulting distribution follows \\(X∼Gamma(kα,θ/k)\\) with persistent mean \\(αθ\\), shrinking variance \\(αθ^2/k\\) and increasing information \\(I_{max} = 1/Var(Y) = 1/(αθ^2/k)\\).\nIn practical terms, as our municipality \\(Y\\) has more fields \\(k\\), we are ever more certain (by the rate of \\(1\\sqrt{k}\\)), that our sample mean \\(E[X_{i}]\\) is the correct risk price.\n\n\n\n\n\n\nMethod\nWe use Monte Carlo simulations under a purely theoretical setup, assuming full independence between units. The steps are:\n\nGenerate samples: For each choice of \\(k\\) (number of fields) and scale \\(θ\\), draw \\(50000\\) independent samples of either \\(Exp(1/θ)\\) or \\(Gamma(α,θ)\\).\nSimulate it for different \\(k\\).\n\ncalculate the empirical parameters like SD and mean\n**Compare to theory: Visually compare the 2 curves, theory and empirical.\n\nOur goal is simply to verify that the simulated averages behave as expected under the Gamma aggregation model.\nNote we talk now about mean, not sum, hence theta gets divided by k\n\\[\\text{mean of } k * Gamma(α, θ)\\Longrightarrow Gamma(α*k, θ/k)\\]\n\n\nSimulation of Exp(1/θ)\nWe run it for 3 different θ:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhy does empirical average diverge at small theta\nWe observe a strange behavour of the empirical average (blue). The problem is only the plotting function after_stat(density). Because it uses Gaussian Kerneling, the near-zero Gaussians leak to &lt;0 and hence distort the smoothing function. If you look at a hist of the empirical average, it looks perfectly gamma.\n\n\n\n\n\n\n\n\n\n\n\n\nSimulation of Gamma(α, θ)\nIt is also thinkable that our atomic risk units (fields) follows a gamma distribution with some \\(α &gt; 1\\), in which case the most likely burn cost will be &gt;0. Thankfully, the aggregation behavour of k Gammas follows the exact same pattern as in the case of Exponentials.\nWe do it for 2 different θ and 2 different α:",
    "crumbs": [
      "Data Modelling",
      "Aggretation of Gammas"
    ]
  },
  {
    "objectID": "modelling/small_sample.html",
    "href": "modelling/small_sample.html",
    "title": "Small Samples",
    "section": "",
    "text": "Aggregation heals mean, not Variance\n\n\n\nIn practice, we have only few years of observation (1-5), which is equivalent to our Monte Carlo with low \\(n_{sim}\\). Our observed data \\(Y\\), however, is composed of a large number of aggregated policies \\(k\\) (certainly &gt;100). We learn that in our setting with small \\(n_{sim}\\) that large \\(k\\) does heal mean, but not variance. In our simulation with \\(n_{sim}=2\\) (2 years), the sample mean quickly convergence to the true mean as \\(k&gt;20\\), but the \\(SD\\) remains completely off. In fact, the variance gets very tight around the (correct) sample mean and completely misses the tail.\n\nCritical\nLarge \\(k\\) cures the mean via the law of large numbers inside each year, but it cannot cure the tail when \\(n_{sim}\\) (years) is small. Intuitively, with \\(n≤5\\) the chance to observe an extreme event (year) is small, no matter how many policies are aggregated. The LLN gives us false confidence and we might drastically underestimate the tail.\n\n\n\n\n\n\n\nMonte Carlo Simulations with small samples\nBelow we now vary the number of MS simulations and observe the effect it has on the estimated mean and variance.\nNote that in our risk model, the \\(n_{sim}\\) corresponds to the number of yearly observations we have for one policy, whereas \\(k\\) signifies the number of policies in an aggregated, observed municipality data point. Our goal here is to understand the effects of both \\(n_{sim}\\) and \\(k\\).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAggregation heals mean, but not variance\nIn practice, we have only few years of observation (1-5), which is equivalent to our monte carlo with few draws. Our data, however, is composed of a large number of aggregated fields (certainly &gt;100), i.e. k is large. It’s important to note that the aggregation effect heals the mean (given i.i.d.), but NOT the variance.\n\n\n\n\n\n\n\n\n\nWorse even… as we increase k, sd gets smaller, but likely wrong.Hence we give ourselves a wrong confidence",
    "crumbs": [
      "Data Modelling",
      "Small Samples"
    ]
  },
  {
    "objectID": "modelling/dependence.html",
    "href": "modelling/dependence.html",
    "title": "Dependence",
    "section": "",
    "text": "Dependence is hard to model\n\n\n\nThe mean stays the same by linearity of Expectations. However, when fields share weather correlation and \\(ρ&gt;0\\), variance reduction stalls; the true distribution becomes a Gamma mixture, where in the extreme cases:\n\n\\(ρ=0\\) variance follows i.i.d law\n\\(ρ=1\\) variance stays unchanged\n\\(ρ≈0.3–0.6\\) we have a large gap between sample and theory.\n\nRelying on the \\(Gamma(kα,θ/k)\\) approximation is safe only when intra-policy correlation is negligible. In our data (mid dependence \\(ρ\\), many policies \\(k\\)) it understates tail risk.\n\nSolution Approach\nWe can think of 2 different approaches: 1. modelling the shared weather factor explicitly 2. Using a hierarchical Bayesian models.\n\n\n\n\n\n\n\nDependency of Fields\nAttention. Our fields (ks) are almost certainly not independent. Likely, the same perils in the municipality are strongly dependent, whereas different perils might be independent from each other. Hence, we have a \\(ρ\\) which is larger than 0 and smaller than 1. This changes the aggregation behaviour and makes the modelling more complicated.\n\n\nDependence Simulation with moment matching\nTo inject dependence we multiply every field in the same year by a common “weather” factor following \\(Gamma\\), whose variance is chosen so that the resulting pair-wise correlation equals the target \\(ρ\\). The theoretical comparisons then use a moment-matched Gamma whose variance is inflated by the factor \\(ρ+(1-ρ)/k\\), giving a simple analytic proxy for the correlated yearly average.\n\nDependencies with large n and k\n\n\n\n\n\n\n\n\n\n\n\nDependencies with large n and small k\n\n\n\n\n\n\n\n\n\n\n\nDependencies with small n and large k\nNote that we chose \\(n = 6\\) because at 2, the picture would be completly messy.\n\n\n\n\n\n\n\n\n\n\n\nDependencies with small n and small k",
    "crumbs": [
      "Data Modelling",
      "Dependence"
    ]
  },
  {
    "objectID": "modelling/mixed_model.html",
    "href": "modelling/mixed_model.html",
    "title": "Mixed Models",
    "section": "",
    "text": "Aggregation Effects and the CLT\n\n\n\nWe want to shed light on the effect of aggregation of a large number of fields. What we do here is we distribute means. Let’s assume \\(X\\) was uniform or beta or any wild distribution, where extreme values are perfectly possible. If we average now \\(k\\) of these \\(X_i\\), then by CLT we obtain a distribution following \\(Normal(αθ, αθ^2/k)\\). This, of course, does not correctly represent the tail risks.\n\n\n\n\n\n\nSummary of Modelling\n\nWe select the gamma-family for risk modelling.\nwe treat each policy as \\(X_{i}=Gamma(α,θ_i)\\); the municipality loss is the yearly average \\(Y_t=\\frac{1}{k}\\sum X_{i,t}\\).\nThis keeps crop/policy heterogeneity (different \\(θ_i\\)) and lets a shared weather factor be layered on top.\nThis prevents from false blowing up n and underestimating risk.\nWe only observe \\(Y\\), the RV \\(X\\) is a unobserved quantity\n\n\n\nWhat we’ve learned\n\nAveraging many fields preserves the mean but shrinks variance only when fields are nearly independent; even mild correlation (\\(ρ≈0.3–0.6\\)) wipes out most information.\nWith very few years, the sample mean “looks fine” while SD and tails are badly under-estimated.\nthe averaging of a large number \\(k\\) of policies in our municipality shrinks the sample mean by the LLN and gives us a false confidence.\n\n\n\nMixture models & the Bayesian link\n\nThe observed \\(Y_t\\) is a mixture of Gamma components (different \\(\\theta_i\\)) multiplied by a latent weather factor; single-family fits miss that shape.\nMixture likelihoods are multimodal and unstable in small samples, but Bayesian priors on weights and parameters regularise the problem and capture full uncertainty.\n\n\n\nWhy move to Bayesian\n\nEncodes the variance-inflation formula \\(\\alpha\\theta^{2}[\\,\\rho+(1-\\rho)/k\\,]\\) directly through hierarchical priors.\nBorrows strength across crops, fields and years—critical when \\(n\\le5\\).\nGives full predictive tails instead of point estimates, aligning with capital-at-risk needs.\nHandles the mixture structure naturally, avoiding the pitfalls of frequentist EM fits.\n\n\n\nExample\nI have a loss ratio uniform between 0-1. We can have n years and k fields. Lets simulate some combinations. The bell-curve’s variance is for the sample means. We compare the example with n=3 and k=50 and vice versa. Both have the same amount of data (150) and both share the same mean. But the tail risk and variance if n=3/k=50 is much larger. The reason is inside-year averaging kills most randomness and all convergece to the mean by CLT. Conversely, when I have large n, then i get a good representation of different and extreme years and hence capture the true randomness.\nWhich is important for us? Clearly the wider. Because we cannot assume that we insure entire municipalities. We insure fields.\n\n\nShow Code\nlibrary(here)\ninvisible(capture.output(source(here(\"scripts\", \"global.R\"))))\n\nCLT_sim_plot &lt;- function(n, k){\n  set.seed(42)\n  # simulate raw draws and yearly averages\n  raw_draws &lt;- runif(n * k) \n  year_id   &lt;- rep(1:n, each = k)\n  avg_year  &lt;- tapply(raw_draws, year_id, mean)\n\n  df_raw &lt;- data.frame(\n  value = raw_draws,\n  year  = factor(year_id))\n\n  df_avg &lt;- data.frame(\n    value = avg_year,\n    what  = \"Yearly average\"\n  )\n  \n  mu &lt;- 0.5\n  sigma_hat &lt;- sqrt(1/12 / k) \n  x_grid &lt;- seq(0, 1, length.out = 400)\n  df_norm &lt;- data.frame(\n    x = x_grid,\n    pdf = dnorm(x_grid, mu, sigma_hat)\n  )\n\n  p&lt;-ggplot() +\n  geom_histogram(data = df_raw, aes(value,  after_stat(density)), binwidth = 0.05, alpha = 0.4, size=0, position = \"identity\", show.legend = FALSE) +\n  geom_density(data = df_avg, aes(value, after_stat(density)), colour = \"steelblue\", size   = 1.2, show.legend = FALSE) +\n  geom_line(data = df_norm, aes(x, pdf), colour   = \"red\",linetype = \"dashed\", size = 1, show.legend = FALSE) +\n  labs(title    = sprintf(\"k = %d per year, n = %d\", k, n),\n       x = \"loss ratio\",\n       y = \"density\",\n       fill   = \"Year\",\n       colour = \"Year\") +\n  theme_minimal() + theme(legend.position=\"none\")\n  return(p)\n}\n\np1 &lt;- CLT_sim_plot(n= 3, k= 3)\np2 &lt;- CLT_sim_plot(n= 50, k= 3)\np3 &lt;- CLT_sim_plot(n= 3, k= 50)\np4 &lt;- CLT_sim_plot(n= 50, k= 50)\n\ncombined_plot &lt;- (p1 + p2) / (p3 + p4) +\n  plot_annotation(\n    title = \"Coloured bars: years | Blue: empirical density | Red dashed: CLT Normal\",\n    theme = theme(\n      legend.position=\"none\",\n      plot.title = element_text(\n        hjust = 0.5, \n\n      )\n    )\n  )\ncombined_plot",
    "crumbs": [
      "Data Modelling",
      "Mixed Models"
    ]
  }
]